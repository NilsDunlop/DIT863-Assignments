{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 2: Naive Bayes Classifier\n",
    "## Group Members:\n",
    "* ### Nils Dunlop, e-mail: gusdunlni@student.gu.se\n",
    "* ### Francisco Alejandro Erazo Piza, e-mail: guserafr@student.gu.se\n",
    "* ### Chukwudumebi Ubogu, e-mail: gusuboch@student.gu.se"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54579eafefd55407"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparatory remarks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c46f3b8c5fc333b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Frequency-counting in Python."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c75c46eb991b4feb"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def read_documents(doc_file):\n",
    "    \"\"\"\n",
    "    Reads a document and returns a list of documents and their corresponding labels.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    labels = []\n",
    "    with open(doc_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            docs.append(words[3:])\n",
    "            labels.append(words[1])\n",
    "    return docs, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:44.775039100Z",
     "start_time": "2023-11-28T11:43:44.481789400Z"
    }
   },
   "id": "25799c9a77429e34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the review data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "432d8b647ce746b2"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "def split_data(doc_file, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Splits the data into training and validation sets.\n",
    "    \"\"\"\n",
    "    all_docs, all_labels = read_documents(doc_file)\n",
    "    split_point = int(train_split * len(all_docs))\n",
    "    train_docs = all_docs[:split_point]\n",
    "    train_labels = all_labels[:split_point]\n",
    "    val_docs = all_docs[split_point:]\n",
    "    val_labels = all_labels[split_point:]\n",
    "    return train_docs, train_labels, val_docs, val_labels, all_docs, all_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:44.873047400Z",
     "start_time": "2023-11-28T11:43:44.495158700Z"
    }
   },
   "id": "74cb4576eb34d477"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estimating parameters for the Naive Bayes classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c55fbd09ecd4e33"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "([('seeks', {'pos': -10.91934950295121, 'neg': -11.604733415225777}),\n  ('curry', {'pos': -12.459794543898358, 'neg': -12.857496383721145}),\n  ('seriousness', {'pos': -12.459794543898358, 'neg': -11.758884095053036}),\n  ('6-month', {'pos': -13.558406832566469, 'neg': -12.857496383721145}),\n  ('37:4', {'pos': -13.558406832566469, 'neg': -12.857496383721145})],\n {'pos': -0.6761896870922498, 'neg': -0.7103971982200179})"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_nb(documents, labels):\n",
    "    \"\"\"\n",
    "    Trains a Naive Bayes classifier given the documents and labels.\n",
    "    Returns a model containing log probabilities.\n",
    "    \"\"\"\n",
    "    # Initialize counters for each class and a counter for all words\n",
    "    pos_counter = Counter()\n",
    "    neg_counter = Counter()\n",
    "    all_words = set()\n",
    "\n",
    "    # Count word frequencies per class\n",
    "    for doc, label in zip(documents, labels):\n",
    "        if label == 'pos':\n",
    "            pos_counter.update(doc)\n",
    "        elif label == 'neg':\n",
    "            neg_counter.update(doc)\n",
    "        all_words.update(doc)\n",
    "\n",
    "    # Total count of words in each class\n",
    "    total_pos = sum(pos_counter.values())\n",
    "    total_neg = sum(neg_counter.values())\n",
    "\n",
    "    # Vocabulary size\n",
    "    V = len(all_words)\n",
    "\n",
    "    # Calculate log probabilities with Laplace smoothing\n",
    "    log_probs = {}\n",
    "    for word in all_words:\n",
    "        # Apply laplace smoothing\n",
    "        log_prob_pos = np.log((pos_counter[word] + 1) / (total_pos + V))\n",
    "        log_prob_neg = np.log((neg_counter[word] + 1) / (total_neg + V))\n",
    "        log_probs[word] = {'pos': log_prob_pos, 'neg': log_prob_neg}\n",
    "\n",
    "    # Calculate the log probabilities of each class\n",
    "    num_pos = sum(1 for l in labels if l == 'pos')\n",
    "    num_neg = sum(1 for l in labels if l == 'neg')\n",
    "\n",
    "    prob_pos = num_pos / len(labels)\n",
    "    prob_neg = num_neg / len(labels)\n",
    "\n",
    "    log_prob_pos_class = np.log(prob_pos)\n",
    "    log_prob_neg_class = np.log(prob_neg)\n",
    "\n",
    "    log_prob_class = {'pos': log_prob_pos_class, 'neg': log_prob_neg_class}\n",
    "    \n",
    "    return log_probs, log_prob_class\n",
    "\n",
    "# Splitting the data and then training the model using the training data\n",
    "train_docs, train_labels, val_docs, val_labels, all_docs, all_labels = split_data('reviews.txt')\n",
    "model, log_prob_class = train_nb(train_docs, train_labels)\n",
    "\n",
    "# Printing 5 instances of the model for verification\n",
    "list(model.items())[:5], log_prob_class"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.535322100Z",
     "start_time": "2023-11-28T11:43:44.513959900Z"
    }
   },
   "id": "650f7e1ff48a8e9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classifying new documents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f892f381087659e"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "def score_doc_label(document, label, model, log_prob_class):\n",
    "    \"\"\"\n",
    "    Computes logarithm probability of the observed words in a document given a sentiment label.\n",
    "    \"\"\"\n",
    "    # Start with the log probability of the label\n",
    "    log_prob = log_prob_class[label]\n",
    "\n",
    "    # Add the log probability of each word in the document\n",
    "    for word in document:\n",
    "        if word in model:\n",
    "            log_prob += model[word][label]\n",
    "        else:\n",
    "            # If the word is not in the model, it's an unseen word, we choose to ignore it\n",
    "            pass\n",
    "\n",
    "    return log_prob"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.551407100Z",
     "start_time": "2023-11-28T11:43:45.525978600Z"
    }
   },
   "id": "549f2794903a9fe3"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.0013212141496043825,\n 0.0005283997934747295,\n 0.00017230368700664423,\n 0.0004547440646873432,\n -12.807858361140351,\n -13.486891735775352)"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check 1: Testing with the word \"great\"\n",
    "log_prob_pos_great = score_doc_label([\"great\"], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_great = score_doc_label([\"great\"], \"neg\", model, log_prob_class)\n",
    "prob_pos_great = np.exp(log_prob_pos_great)\n",
    "prob_neg_great = np.exp(log_prob_neg_great)\n",
    "\n",
    "# Sanity Check 2: Testing with the word \"bad\"\n",
    "log_prob_pos_bad = score_doc_label([\"bad\"], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_bad = score_doc_label([\"bad\"], \"neg\", model, log_prob_class)\n",
    "prob_pos_bad = np.exp(log_prob_pos_bad)\n",
    "prob_neg_bad = np.exp(log_prob_neg_bad)\n",
    "\n",
    "# Sanity Check 3: Testing with the document ['a', 'top-quality', 'performance']\n",
    "log_prob_pos_doc = score_doc_label(['a', 'top-quality', 'performance'], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_doc = score_doc_label(['a', 'top-quality', 'performance'], \"neg\", model, log_prob_class)\n",
    "\n",
    "prob_pos_great, prob_neg_great, prob_pos_bad, prob_neg_bad, log_prob_pos_doc, log_prob_neg_doc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.552410200Z",
     "start_time": "2023-11-28T11:43:45.541857100Z"
    }
   },
   "id": "1f9021e224ce387e"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "def classify_nb(document, model, log_prob_class):\n",
    "    \"\"\"\n",
    "    Classify a new document using the Naive Bayes classifier.\n",
    "    \"\"\"\n",
    "    # Compute the log probability for each class\n",
    "    log_prob_pos = score_doc_label(document, \"pos\", model, log_prob_class)\n",
    "    log_prob_neg = score_doc_label(document, \"neg\", model, log_prob_class)\n",
    "\n",
    "    # Return the class with the higher log probability\n",
    "    if log_prob_pos > log_prob_neg:\n",
    "        return \"pos\"\n",
    "    else:\n",
    "        return \"neg\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.625536400Z",
     "start_time": "2023-11-28T11:43:45.557407Z"
    }
   },
   "id": "ece68b5805521e7f"
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "[(['great'], 'pos'),\n (['bad'], 'neg'),\n (['amazing'], 'pos'),\n (['terrible'], 'neg'),\n (['a', 'top-quality', 'performance'], 'pos')]"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on small test documents\n",
    "test_docs = [[\"great\"], [\"bad\"], [\"amazing\"], [\"terrible\"], ['a', 'top-quality', 'performance']]\n",
    "\n",
    "# Applying the classify_nb function to the test documents\n",
    "classified_docs = []\n",
    "for doc in test_docs:\n",
    "    classification = classify_nb(doc, model, log_prob_class)\n",
    "    classified_docs.append((doc, classification))\n",
    "\n",
    "classified_docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.640066900Z",
     "start_time": "2023-11-28T11:43:45.572453300Z"
    }
   },
   "id": "6571dc86a2f79bb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating the classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ac9440a37b0840f"
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "def classify_documents(docs, model, log_prob_class):\n",
    "    \"\"\"\n",
    "    Classifies documents in the provided collection.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for doc in docs:\n",
    "        prediction = classify_nb(doc, model, log_prob_class)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.642049200Z",
     "start_time": "2023-11-28T11:43:45.585977100Z"
    }
   },
   "id": "9ebe9a960a606f2a"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "def accuracy(true_labels, guessed_labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the classifier.\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "\n",
    "    for t, g in zip(true_labels, guessed_labels):\n",
    "        if t == g:\n",
    "            correct_count += 1\n",
    "    \n",
    "    acc = correct_count / len(true_labels)\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.643044600Z",
     "start_time": "2023-11-28T11:43:45.604005800Z"
    }
   },
   "id": "6f7407e97440fe9"
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "def precision_recall_f1(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "\n",
    "    # Counting true positives, false positives, and false negatives\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        if true == 'pos' and pred == 'pos':\n",
    "            true_positives += 1\n",
    "        elif true == 'neg' and pred == 'pos':\n",
    "            false_positives += 1\n",
    "        elif true == 'pos' and pred == 'neg':\n",
    "            false_negatives += 1\n",
    "\n",
    "    # Calculating precision, recall, and F1 score\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "\n",
    "    if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    if true_positives + false_negatives > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.644551800Z",
     "start_time": "2023-11-28T11:43:45.618060100Z"
    }
   },
   "id": "6f1edf91b0fa4af8"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8153587914393622 \n",
      "Precision: 0.8237965485921889\n",
      "Recall: 0.7866435385949696\n",
      "F1 score: 0.8047914818101153\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy, precision, recall, and F1 score\n",
    "predicted_labels = classify_documents(val_docs, model, log_prob_class)\n",
    "accuracy_result = accuracy(val_labels, predicted_labels)\n",
    "precision, recall, f1_score = precision_recall_f1(val_labels, predicted_labels)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_result} \\nPrecision: {precision}\\nRecall: {recall}\\nF1 score: {f1_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.835625800Z",
     "start_time": "2023-11-28T11:43:45.643044600Z"
    }
   },
   "id": "c693470fd9e199ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is the difference between F1 score and accuracy?\n",
    "\n",
    "**Accuracy**\n",
    "Accuracy is a measures the proportion of correctly predicted instances out of all the predictions made. In short, it is the number of correct predictions divided by the total number of predictions. Accuracy gives us a general idea of how well the model is performing.\n",
    "\n",
    "**Precision**\n",
    "F1 score is a slightly more nuanced metric. It is the harmonic mean of precision and recall. Precision is the ratio of true positives to all positive predictions. Recall is the ratio of true positives to all actual positive instances. F1 score balances precision and recall, providing a more holistic view of how well the model is performing especially when the classes are imbalanced."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11e1cbab3ac8e1e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Error Analysis\n",
    "We analyzed the first 10 documents and their corresponding prediction. Below you will see three instances which were harder to predict: \n",
    " \n",
    "1. Document about an Iron **(Predicted: Negative)**:\n",
    "\n",
    "* Content: The document starts with a warning not to buy the iron, mentions it's fabulous when it works, but then highlights that it broke in less than a week and had other issues.\n",
    "* Analysis: The prediction as negative seems correct. The initial positive remark \"it's fabulous\" probably was outweighed by the predominantly negative context.\n",
    "\n",
    "2. Document about Madame Bovary **(Predicted: Positive)**:\n",
    "* Content: The review seems to be more about the characters in the story, with a mix of critique and appreciation for the adaptation and performances.\n",
    "* Analysis: This document was tricky to label as it wrote polarizing thoughts about the book. We assume that the classifier picked up on more positive aspects than negative aspects of the book, leading to a positive prediction. \n",
    "\n",
    "3. Document about Microsoft Office **(Predicted: Negative)**:\n",
    "* Content: The reviewer criticizes the new interface of Microsoft Office, finding it infuriating and less efficient.\n",
    "* Analysis: The negative prediction aligns with the strong criticism expressed in the document.\n",
    "\n",
    "Overall the classifier seemed to predict well the sentiment expressed in the first 10 documents. The more challenging cases for the classifier could be the ones with mixed sentiments or when the sentiment is expressed in a not so straightforward way such as the Madame Bovary document."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1eea5da76cf058"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross-validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83a06e0e48484bdb"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "def cross_validation_10_fold(all_docs, all_labels, N=10):\n",
    "    \"\"\"\n",
    "    Performs 10-fold cross-validation. \n",
    "    \"\"\"\n",
    "    accuracy_scores = []\n",
    "    for fold_nbr in range(N):\n",
    "        split_point_1 = int(float(fold_nbr)/N*len(all_docs))\n",
    "        split_point_2 = int(float(fold_nbr+1)/N*len(all_docs))\n",
    "\n",
    "        train_docs_fold = all_docs[:split_point_1] + all_docs[split_point_2:]\n",
    "        train_labels_fold = all_labels[:split_point_1] + all_labels[split_point_2:]\n",
    "        val_docs_fold = all_docs[split_point_1:split_point_2]\n",
    "        val_labels_fold = all_labels[split_point_1:split_point_2]\n",
    "\n",
    "        model, log_prob_class = train_nb(train_docs_fold, train_labels_fold)\n",
    "        predicted_labels = classify_documents(val_docs_fold, model, log_prob_class)\n",
    "        accuracy_scores.append(accuracy(val_labels_fold, predicted_labels))\n",
    "\n",
    "    return sum(accuracy_scores) / N"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.875609700Z",
     "start_time": "2023-11-28T11:43:45.838973500Z"
    }
   },
   "id": "50aed86d493a1946"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "def cross_validation_loocv(all_docs, all_labels, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Performs leave-one-out cross-validation. \n",
    "    \"\"\"\n",
    "    accuracy_scores = []\n",
    "    for i in range(min(max_iterations, len(all_docs))):\n",
    "        train_docs_fold = all_docs[:i] + all_docs[i+1:]\n",
    "        train_labels_fold = all_labels[:i] + all_labels[i+1:]\n",
    "        val_docs_fold = [all_docs[i]]\n",
    "        val_labels_fold = [all_labels[i]]\n",
    "\n",
    "        model, log_prob_class = train_nb(train_docs_fold, train_labels_fold)\n",
    "        predicted_labels = classify_documents(val_docs_fold, model, log_prob_class)\n",
    "        accuracy_scores.append(accuracy(val_labels_fold, predicted_labels))\n",
    "\n",
    "    return sum(accuracy_scores) / len(accuracy_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:43:45.887122600Z",
     "start_time": "2023-11-28T11:43:45.849479200Z"
    }
   },
   "id": "8218746010ae360d"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (10-Fold Cross-Validation): 0.8076216900805256\n",
      "Average Accuracy (LOOCV, first 100 iterations): 0.83\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_accuracy_10_fold = cross_validation_10_fold(all_docs, all_labels, N=10)\n",
    "print(\"Average Accuracy (10-Fold Cross-Validation):\", avg_accuracy_10_fold)\n",
    "\n",
    "# LOOCV\n",
    "avg_accuracy_loocv = cross_validation_loocv(all_docs, all_labels, max_iterations=100)\n",
    "print(\"Average Accuracy (LOOCV, first 100 iterations):\", avg_accuracy_loocv)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T11:44:46.366370900Z",
     "start_time": "2023-11-28T11:43:45.867017800Z"
    }
   },
   "id": "54db05022a6506a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross-validation results\n",
    "* **10-Fold Cross-Validation**: Achieved an average accuracy of about 0.808. This method offers a balanced evaluation, as it trains and validates across different subsets of the data resulting in a consistent classifier performance.\n",
    "* **LOOCV (100 Iterations)**: Showed a slightly higher average accuracy of 0.83. By training on nearly all available data for each test case, this method offers a more accurate evaluation of the classifier performance. However, it is computationally expensive and can be slow to run.\n",
    "\n",
    "The 10-fold and LOOCV results indicate a reliable classifier, with 10-fold offering a broader accuracy measure and LOOCV providing more detailed validation, however with higher computational demands."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54036a807a14e073"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Domain sensitivity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cf29ef57f0f03dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5f67b2bb402e3769"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
