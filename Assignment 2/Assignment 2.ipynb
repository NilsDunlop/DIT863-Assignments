{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def read_documents(doc_file):\n",
    "    \"\"\"\n",
    "    Reads a document and returns a list of documents and their corresponding labels.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    labels = []\n",
    "    with open(doc_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            docs.append(words[3:])\n",
    "            labels.append(words[1])\n",
    "    return docs, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:39:01.768508700Z",
     "start_time": "2023-11-27T13:39:01.758972200Z"
    }
   },
   "id": "25799c9a77429e34"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def split_data(doc_file, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Splits the data into training and validation sets.\n",
    "    \"\"\"\n",
    "    all_docs, all_labels = read_documents(doc_file)\n",
    "    split_point = int(train_split * len(all_docs))\n",
    "    train_docs = all_docs[:split_point]\n",
    "    train_labels = all_labels[:split_point]\n",
    "    val_docs = all_docs[split_point:]\n",
    "    val_labels = all_labels[split_point:]\n",
    "    return train_docs, train_labels, val_docs, val_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:39:01.810201500Z",
     "start_time": "2023-11-27T13:39:01.771531900Z"
    }
   },
   "id": "74cb4576eb34d477"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "([('scripter', {'pos': -13.558406832566469, 'neg': -12.857496383721145}),\n  ('total', {'pos': -9.569422786002194, 'neg': -8.788469629483334}),\n  ('cued', {'pos': -13.558406832566469, 'neg': -12.857496383721145}),\n  ('28-300', {'pos': -12.865259652006523, 'neg': -13.55064356428109}),\n  ('mp3.', {'pos': -12.459794543898358, 'neg': -11.604733415225777})],\n {'pos': -0.6761896870922498, 'neg': -0.7103971982200179})"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_nb(documents, labels):\n",
    "    \"\"\"\n",
    "    Trains a Naive Bayes classifier given the documents and labels.\n",
    "    Returns a model containing log probabilities.\n",
    "    \"\"\"\n",
    "    # Initialize counters for each class and a counter for all words\n",
    "    pos_counter = Counter()\n",
    "    neg_counter = Counter()\n",
    "    all_words = set()\n",
    "\n",
    "    # Count word frequencies per class\n",
    "    for doc, label in zip(documents, labels):\n",
    "        if label == 'pos':\n",
    "            pos_counter.update(doc)\n",
    "        elif label == 'neg':\n",
    "            neg_counter.update(doc)\n",
    "        all_words.update(doc)\n",
    "\n",
    "    # Total count of words in each class\n",
    "    total_pos = sum(pos_counter.values())\n",
    "    total_neg = sum(neg_counter.values())\n",
    "\n",
    "    # Vocabulary size for Laplace smoothing\n",
    "    V = len(all_words)\n",
    "\n",
    "    # Calculate log probabilities with Laplace smoothing\n",
    "    log_probs = {}\n",
    "    for word in all_words:\n",
    "        # Apply laplace smoothing\n",
    "        log_prob_pos = np.log((pos_counter[word] + 1) / (total_pos + V))\n",
    "        log_prob_neg = np.log((neg_counter[word] + 1) / (total_neg + V))\n",
    "        log_probs[word] = {'pos': log_prob_pos, 'neg': log_prob_neg}\n",
    "\n",
    "    # Calculate the log probabilities of each class\n",
    "    num_pos = sum(1 for l in labels if l == 'pos')\n",
    "    num_neg = sum(1 for l in labels if l == 'neg')\n",
    "\n",
    "    prob_pos = num_pos / len(labels)\n",
    "    prob_neg = num_neg / len(labels)\n",
    "\n",
    "    log_prob_pos_class = np.log(prob_pos)\n",
    "    log_prob_neg_class = np.log(prob_neg)\n",
    "\n",
    "    log_prob_class = {'pos': log_prob_pos_class, 'neg': log_prob_neg_class}\n",
    "    \n",
    "    return log_probs, log_prob_class\n",
    "\n",
    "# Splitting the data and then training the model using the training data\n",
    "train_docs, train_labels, val_docs, val_labels = split_data('reviews.txt')\n",
    "model, log_prob_class = train_nb(train_docs, train_labels)\n",
    "\n",
    "# Printing 5 instances of the model for verification\n",
    "list(model.items())[:5], log_prob_class"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:39:02.633305600Z",
     "start_time": "2023-11-27T13:39:01.787070600Z"
    }
   },
   "id": "650f7e1ff48a8e9e"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def score_doc_label(document, label, model, log_prob_class):\n",
    "    \"\"\"\n",
    "    Computes logarithm probability of the observed words in a document given a sentiment label.\n",
    "    \"\"\"\n",
    "    # Start with the log probability of the label\n",
    "    log_prob = log_prob_class[label]\n",
    "\n",
    "    # Add the log probability of each word in the document\n",
    "    for word in document:\n",
    "        if word in model:\n",
    "            log_prob += model[word][label]\n",
    "        else:\n",
    "            # If the word is not in the model, it's an unseen word, we choose to ignore it\n",
    "            pass\n",
    "\n",
    "    return log_prob"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:39:02.648084100Z",
     "start_time": "2023-11-27T13:39:02.636791200Z"
    }
   },
   "id": "549f2794903a9fe3"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.0013212141496043825,\n 0.0005283997934747295,\n 0.00017230368700664423,\n 0.0004547440646873432,\n -12.807858361140351,\n -13.486891735775352)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check 1: Testing with the word \"great\"\n",
    "log_prob_pos_great = score_doc_label([\"great\"], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_great = score_doc_label([\"great\"], \"neg\", model, log_prob_class)\n",
    "prob_pos_great = np.exp(log_prob_pos_great)\n",
    "prob_neg_great = np.exp(log_prob_neg_great)\n",
    "\n",
    "# Sanity Check 2: Testing with the word \"bad\"\n",
    "log_prob_pos_bad = score_doc_label([\"bad\"], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_bad = score_doc_label([\"bad\"], \"neg\", model, log_prob_class)\n",
    "prob_pos_bad = np.exp(log_prob_pos_bad)\n",
    "prob_neg_bad = np.exp(log_prob_neg_bad)\n",
    "\n",
    "# Sanity Check 3: Testing with the document ['a', 'top-quality', 'performance']\n",
    "log_prob_pos_doc = score_doc_label(['a', 'top-quality', 'performance'], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_doc = score_doc_label(['a', 'top-quality', 'performance'], \"neg\", model, log_prob_class)\n",
    "\n",
    "prob_pos_great, prob_neg_great, prob_pos_bad, prob_neg_bad, log_prob_pos_doc, log_prob_neg_doc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:39:02.705557Z",
     "start_time": "2023-11-27T13:39:02.652096500Z"
    }
   },
   "id": "1f9021e224ce387e"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def classify_nb(document, model, log_prob_class):\n",
    "    \"\"\"\n",
    "    Classify a new document using the Naive Bayes classifier.\n",
    "    \"\"\"\n",
    "    # Compute the log probability for each class\n",
    "    log_prob_pos = score_doc_label(document, \"pos\", model, log_prob_class)\n",
    "    log_prob_neg = score_doc_label(document, \"neg\", model, log_prob_class)\n",
    "\n",
    "    # Return the class with the higher log probability\n",
    "    if log_prob_pos > log_prob_neg:\n",
    "        return \"pos\"\n",
    "    else:\n",
    "        return \"neg\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:39:02.714133900Z",
     "start_time": "2023-11-27T13:39:02.664623100Z"
    }
   },
   "id": "ece68b5805521e7f"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[(['great'], 'pos'),\n (['bad'], 'neg'),\n (['amazing'], 'pos'),\n (['terrible'], 'neg'),\n (['a', 'top-quality', 'performance'], 'pos')]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on small test documents\n",
    "test_docs = [[\"great\"], [\"bad\"], [\"amazing\"], [\"terrible\"], ['a', 'top-quality', 'performance']]\n",
    "\n",
    "# Applying the classify_nb function to the test documents\n",
    "classified_docs = []\n",
    "for doc in test_docs:\n",
    "    classification = classify_nb(doc, model, log_prob_class)\n",
    "    classified_docs.append((doc, classification))\n",
    "\n",
    "classified_docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T13:39:02.715121600Z",
     "start_time": "2023-11-27T13:39:02.682430200Z"
    }
   },
   "id": "6571dc86a2f79bb6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
