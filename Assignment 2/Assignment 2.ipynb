{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def read_documents(doc_file):\n",
    "    \"\"\"\n",
    "    Reads a document and returns a list of documents and their corresponding labels.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    labels = []\n",
    "    with open(doc_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            docs.append(words[3:])\n",
    "            labels.append(words[1])\n",
    "    return docs, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:16.980883500Z",
     "start_time": "2023-11-27T17:25:16.746837600Z"
    }
   },
   "id": "25799c9a77429e34"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def split_data(doc_file, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Splits the data into training and validation sets.\n",
    "    \"\"\"\n",
    "    all_docs, all_labels = read_documents(doc_file)\n",
    "    split_point = int(train_split * len(all_docs))\n",
    "    train_docs = all_docs[:split_point]\n",
    "    train_labels = all_labels[:split_point]\n",
    "    val_docs = all_docs[split_point:]\n",
    "    val_labels = all_labels[split_point:]\n",
    "    return train_docs, train_labels, val_docs, val_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:16.986717900Z",
     "start_time": "2023-11-27T17:25:16.776623300Z"
    }
   },
   "id": "74cb4576eb34d477"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "([('seeks', {'pos': -10.91934950295121, 'neg': -11.604733415225777}),\n  ('curry', {'pos': -12.459794543898358, 'neg': -12.857496383721145}),\n  ('seriousness', {'pos': -12.459794543898358, 'neg': -11.758884095053036}),\n  ('6-month', {'pos': -13.558406832566469, 'neg': -12.857496383721145}),\n  ('37:4', {'pos': -13.558406832566469, 'neg': -12.857496383721145})],\n {'pos': -0.6761896870922498, 'neg': -0.7103971982200179})"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_nb(documents, labels):\n",
    "    \"\"\"\n",
    "    Trains a Naive Bayes classifier given the documents and labels.\n",
    "    Returns a model containing log probabilities.\n",
    "    \"\"\"\n",
    "    # Initialize counters for each class and a counter for all words\n",
    "    pos_counter = Counter()\n",
    "    neg_counter = Counter()\n",
    "    all_words = set()\n",
    "\n",
    "    # Count word frequencies per class\n",
    "    for doc, label in zip(documents, labels):\n",
    "        if label == 'pos':\n",
    "            pos_counter.update(doc)\n",
    "        elif label == 'neg':\n",
    "            neg_counter.update(doc)\n",
    "        all_words.update(doc)\n",
    "\n",
    "    # Total count of words in each class\n",
    "    total_pos = sum(pos_counter.values())\n",
    "    total_neg = sum(neg_counter.values())\n",
    "\n",
    "    # Vocabulary size for Laplace smoothing\n",
    "    V = len(all_words)\n",
    "\n",
    "    # Calculate log probabilities with Laplace smoothing\n",
    "    log_probs = {}\n",
    "    for word in all_words:\n",
    "        # Apply laplace smoothing\n",
    "        log_prob_pos = np.log((pos_counter[word] + 1) / (total_pos + V))\n",
    "        log_prob_neg = np.log((neg_counter[word] + 1) / (total_neg + V))\n",
    "        log_probs[word] = {'pos': log_prob_pos, 'neg': log_prob_neg}\n",
    "\n",
    "    # Calculate the log probabilities of each class\n",
    "    num_pos = sum(1 for l in labels if l == 'pos')\n",
    "    num_neg = sum(1 for l in labels if l == 'neg')\n",
    "\n",
    "    prob_pos = num_pos / len(labels)\n",
    "    prob_neg = num_neg / len(labels)\n",
    "\n",
    "    log_prob_pos_class = np.log(prob_pos)\n",
    "    log_prob_neg_class = np.log(prob_neg)\n",
    "\n",
    "    log_prob_class = {'pos': log_prob_pos_class, 'neg': log_prob_neg_class}\n",
    "    \n",
    "    return log_probs, log_prob_class\n",
    "\n",
    "# Splitting the data and then training the model using the training data\n",
    "train_docs, train_labels, val_docs, val_labels = split_data('reviews.txt')\n",
    "model, log_prob_class = train_nb(train_docs, train_labels)\n",
    "\n",
    "# Printing 5 instances of the model for verification\n",
    "list(model.items())[:5], log_prob_class"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.109437100Z",
     "start_time": "2023-11-27T17:25:16.791687300Z"
    }
   },
   "id": "650f7e1ff48a8e9e"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def score_doc_label(document, label, model, log_prob_class):\n",
    "    \"\"\"\n",
    "    Computes logarithm probability of the observed words in a document given a sentiment label.\n",
    "    \"\"\"\n",
    "    # Start with the log probability of the label\n",
    "    log_prob = log_prob_class[label]\n",
    "\n",
    "    # Add the log probability of each word in the document\n",
    "    for word in document:\n",
    "        if word in model:\n",
    "            log_prob += model[word][label]\n",
    "        else:\n",
    "            # If the word is not in the model, it's an unseen word, we choose to ignore it\n",
    "            pass\n",
    "\n",
    "    return log_prob"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.151131800Z",
     "start_time": "2023-11-27T17:25:18.115667700Z"
    }
   },
   "id": "549f2794903a9fe3"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.0013212141496043825,\n 0.0005283997934747295,\n 0.00017230368700664423,\n 0.0004547440646873432,\n -12.807858361140351,\n -13.486891735775352)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check 1: Testing with the word \"great\"\n",
    "log_prob_pos_great = score_doc_label([\"great\"], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_great = score_doc_label([\"great\"], \"neg\", model, log_prob_class)\n",
    "prob_pos_great = np.exp(log_prob_pos_great)\n",
    "prob_neg_great = np.exp(log_prob_neg_great)\n",
    "\n",
    "# Sanity Check 2: Testing with the word \"bad\"\n",
    "log_prob_pos_bad = score_doc_label([\"bad\"], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_bad = score_doc_label([\"bad\"], \"neg\", model, log_prob_class)\n",
    "prob_pos_bad = np.exp(log_prob_pos_bad)\n",
    "prob_neg_bad = np.exp(log_prob_neg_bad)\n",
    "\n",
    "# Sanity Check 3: Testing with the document ['a', 'top-quality', 'performance']\n",
    "log_prob_pos_doc = score_doc_label(['a', 'top-quality', 'performance'], \"pos\", model, log_prob_class)\n",
    "log_prob_neg_doc = score_doc_label(['a', 'top-quality', 'performance'], \"neg\", model, log_prob_class)\n",
    "\n",
    "prob_pos_great, prob_neg_great, prob_pos_bad, prob_neg_bad, log_prob_pos_doc, log_prob_neg_doc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.172766300Z",
     "start_time": "2023-11-27T17:25:18.132614100Z"
    }
   },
   "id": "1f9021e224ce387e"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def classify_nb(document, model, log_prob_class):\n",
    "    \"\"\"\n",
    "    Classify a new document using the Naive Bayes classifier.\n",
    "    \"\"\"\n",
    "    # Compute the log probability for each class\n",
    "    log_prob_pos = score_doc_label(document, \"pos\", model, log_prob_class)\n",
    "    log_prob_neg = score_doc_label(document, \"neg\", model, log_prob_class)\n",
    "\n",
    "    # Return the class with the higher log probability\n",
    "    if log_prob_pos > log_prob_neg:\n",
    "        return \"pos\"\n",
    "    else:\n",
    "        return \"neg\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.173738900Z",
     "start_time": "2023-11-27T17:25:18.160182200Z"
    }
   },
   "id": "ece68b5805521e7f"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "[(['great'], 'pos'),\n (['bad'], 'neg'),\n (['amazing'], 'pos'),\n (['terrible'], 'neg'),\n (['a', 'top-quality', 'performance'], 'pos')]"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on small test documents\n",
    "test_docs = [[\"great\"], [\"bad\"], [\"amazing\"], [\"terrible\"], ['a', 'top-quality', 'performance']]\n",
    "\n",
    "# Applying the classify_nb function to the test documents\n",
    "classified_docs = []\n",
    "for doc in test_docs:\n",
    "    classification = classify_nb(doc, model, log_prob_class)\n",
    "    classified_docs.append((doc, classification))\n",
    "\n",
    "classified_docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.252715500Z",
     "start_time": "2023-11-27T17:25:18.180264500Z"
    }
   },
   "id": "6571dc86a2f79bb6"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def classify_documents(docs, model, log_prob_class):\n",
    "    \"\"\"\n",
    "    Classifies documents in the provided collection.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for doc in docs:\n",
    "        prediction = classify_nb(doc, model, log_prob_class)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.253719200Z",
     "start_time": "2023-11-27T17:25:18.194829400Z"
    }
   },
   "id": "9ebe9a960a606f2a"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def accuracy(true_labels, guessed_labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the classifier.\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "\n",
    "    for t, g in zip(true_labels, guessed_labels):\n",
    "        if t == g:\n",
    "            correct_count += 1\n",
    "    \n",
    "    acc = correct_count / len(true_labels)\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.254713700Z",
     "start_time": "2023-11-27T17:25:18.203364900Z"
    }
   },
   "id": "6f7407e97440fe9"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def precision_recall_f1(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "\n",
    "    # Counting true positives, false positives, and false negatives\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        if true == 'pos' and pred == 'pos':\n",
    "            true_positives += 1\n",
    "        elif true == 'neg' and pred == 'pos':\n",
    "            false_positives += 1\n",
    "        elif true == 'pos' and pred == 'neg':\n",
    "            false_negatives += 1\n",
    "\n",
    "    # Calculating precision, recall, and F1 score\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "\n",
    "    if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    if true_positives + false_negatives > 0:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.255726300Z",
     "start_time": "2023-11-27T17:25:18.225111900Z"
    }
   },
   "id": "6f1edf91b0fa4af8"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8153587914393622 \n",
      "Precision: 0.8237965485921889\n",
      "Recall: 0.7866435385949696\n",
      "F1 score: 0.8047914818101153\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy, precision, recall, and F1 score\n",
    "predicted_labels = classify_documents(val_docs, model, log_prob_class)\n",
    "accuracy_result = accuracy(val_labels, predicted_labels)\n",
    "precision, recall, f1_score = precision_recall_f1(val_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_result} \\nPrecision: {precision}\\nRecall: {recall}\\nF1 score: {f1_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.536295900Z",
     "start_time": "2023-11-27T17:25:18.237650500Z"
    }
   },
   "id": "c693470fd9e199ef"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T17:25:18.548368400Z",
     "start_time": "2023-11-27T17:25:18.534292400Z"
    }
   },
   "id": "86390cd10eba3b46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
